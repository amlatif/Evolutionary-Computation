{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac1b57b5-7544-4b85-b400-fc76af45dda0",
   "metadata": {},
   "source": [
    "**Hill Climbing** is a simple and widely used **local search optimization algorithm** that aims to find the best solution (or a near-optimal solution) to an optimization problem. It is inspired by the metaphor of climbing a hill: starting at a random point, you iteratively move \"uphill\" toward higher ground (better solutions) until you reach the peak (a local or global optimum).\n",
    "\n",
    "\n",
    "The algorithm starts with an initial solution and iteratively explores neighboring solutions in the search space. At each step, it evaluates the objective function (fitness) of the current solution and its neighbors, moving to the neighbor with the highest value (in maximization problems) or the lowest value (in minimization problems). The process continues until no improving neighbor can be found, at which point the algorithm terminates.\n",
    "\n",
    "---\n",
    "\n",
    "### General Hill Climbing Algorithm (Pseudocode):\n",
    "```plaintext\n",
    "function HillClimbing(problem):\n",
    "    current = initialState(problem)\n",
    "    while True:\n",
    "        neighbors = generateNeighbors(current)\n",
    "        bestNeighbor = None\n",
    "        bestValue = evaluate(current)\n",
    "        \n",
    "        for neighbor in neighbors:\n",
    "            neighborValue = evaluate(neighbor)\n",
    "            if neighborValue > bestValue:  # For maximization problems\n",
    "                bestNeighbor = neighbor\n",
    "                bestValue = neighborValue\n",
    "        \n",
    "        if bestNeighbor is None:  # No better neighbor found\n",
    "            return current  # Local optimum reached\n",
    "        else:\n",
    "            current = bestNeighbor  # Move to the best neighbor\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Variants of Hill Climbing:\n",
    "1. **Simple Hill Climbing**:\n",
    "   - Moves to the first neighbor that improves the current solution.\n",
    "   - Stops as soon as an improvement is found, without evaluating all neighbors.\n",
    "   - Fast but prone to getting stuck in poor local optima.\n",
    "\n",
    "2. **Steepest-Ascent Hill Climbing**:\n",
    "   - Evaluates all neighbors and moves to the one with the highest improvement.\n",
    "   - More thorough than Simple Hill Climbing but computationally more expensive.\n",
    "\n",
    "3. **Random Mutation Hill Climbing**:\n",
    "   - Introduces randomness by generating random mutations of the current solution.\n",
    "   - Accepts the mutation only if it improves the solution.\n",
    "   - Helps escape local optima but may take longer to converge.\n",
    "\n",
    "4. **Random-Restart Hill Climbing**:\n",
    "   - Repeatedly restarts the Hill Climbing algorithm from new random initial solutions.\n",
    "   - Increases the likelihood of finding the global optimum by exploring multiple regions of the search space.\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages of Hill Climbing:\n",
    "1. **Simplicity**: The algorithm is easy to understand and implement.\n",
    "2. **Efficiency**: It requires minimal computational resources, especially in small or smooth search spaces.\n",
    "3. **Incremental Improvement**: Progressively improves the solution, making it suitable for real-time or interactive applications.\n",
    "\n",
    "---\n",
    "\n",
    "### Disadvantages of Hill Climbing:\n",
    "1. **Local Optima**: The algorithm can get stuck in suboptimal solutions (local optima) and fail to find the global optimum.\n",
    "2. **Plateaus and Ridges**:\n",
    "   - **Plateaus**: Regions where the objective function is flat, making it difficult to determine the direction of improvement.\n",
    "   - **Ridges**: Narrow paths of improvement that require precise movements, which the algorithm may struggle to follow.\n",
    "3. **Dependence on Initial Solution**: The quality of the final solution depends heavily on the starting point.\n",
    "4. **Lack of Global Exploration**: The algorithm focuses only on local improvements and does not explore the search space systematically.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Hill Climbing:\n",
    "- **Small Search Spaces**: When the search space is small or well-behaved, Hill Climbing can efficiently find good solutions.\n",
    "- **Smooth Landscapes**: In problems with few local optima or plateaus, Hill Climbing performs well.\n",
    "- **Real-Time Applications**: When quick, incremental improvements are needed, such as in robotics or game AI.\n",
    "\n",
    "---\n",
    "\n",
    "### Extensions and Enhancements:\n",
    "To address the limitations of basic Hill Climbing, several advanced techniques have been developed:\n",
    "1. **Simulated Annealing**:\n",
    "   - Allows occasional moves to worse solutions to escape local optima.\n",
    "   - Uses a temperature parameter that decreases over time to control exploration vs. exploitation.\n",
    "2. **Genetic Algorithms**:\n",
    "   - Combines multiple solutions through crossover and mutation to explore the search space globally.\n",
    "3. **Tabu Search**:\n",
    "   - Maintains a memory of recently visited solutions to avoid revisiting them.\n",
    "4. **Hybrid Approaches**:\n",
    "   - Combine Hill Climbing with other algorithms (e.g., random restarts, simulated annealing) to balance exploration and exploitation.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a0bf7b-38f5-4ff5-adc6-43bbaf578799",
   "metadata": {},
   "source": [
    "### General Steps for Maximizing a Function with Hill Climbing:\n",
    "1. **Define the Objective Function**:\n",
    "   - The function $ f(x) $ that you want to maximize.\n",
    "   \n",
    "2. **Choose an Initial Solution**:\n",
    "   - Start with a random or predefined initial point $ x_0 $ in the domain of the function.\n",
    "\n",
    "3. **Generate Neighbors**:\n",
    "   - Define a neighborhood structure around the current solution. For example, in one-dimensional problems, neighbors could be $ x + \\Delta x $ and $ x - \\Delta x $, where $ \\Delta x $ is a small step size.\n",
    "\n",
    "4. **Evaluate the Objective Function**:\n",
    "   - Compute the value of $ f(x) $ for the current solution and its neighbors.\n",
    "\n",
    "5. **Move to the Best Neighbor**:\n",
    "   - If any neighbor has a higher value of $ f(x) $, move to that neighbor. Otherwise, terminate (you've reached a local maximum).\n",
    "\n",
    "6. **Repeat**:\n",
    "   - Continue the process until no improving neighbor can be found.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Maximizing a 1D Function\n",
    "Letâ€™s maximize the function $ f(x) = -(x^2 - 4x + 4) $ over the range $ x \\in [0, 5] $. This function simplifies to $ f(x) = -(x-2)^2 $, which has a global maximum at $ x = 2 $, where $ f(2) = 0 $.\n",
    "\n",
    "#### Step-by-Step Process:\n",
    "\n",
    "1. **Initial Solution**:\n",
    "   - Start with an initial guess, say $ x_0 = 1 $.\n",
    "\n",
    "2. **Generate Neighbors**:\n",
    "   - Define neighbors as $ x + \\Delta x $ and $ x - \\Delta x $, where $ \\Delta x = 0.1 $.\n",
    "   - For $ x = 1 $, the neighbors are:\n",
    "     - $ x_{\\text{left}} = 1 - 0.1 = 0.9 $\n",
    "     - $ x_{\\text{right}} = 1 + 0.1 = 1.1 $\n",
    "\n",
    "3. **Evaluate the Objective Function**:\n",
    "   - Compute $ f(x) $ for the current solution and its neighbors:\n",
    "     - $ f(1) = -(1^2 - 4(1) + 4) = -(1 - 4 + 4) = -1 $\n",
    "     - $ f(0.9) = -(0.9^2 - 4(0.9) + 4) = -(0.81 - 3.6 + 4) = -0.21 $\n",
    "     - $ f(1.1) = -(1.1^2 - 4(1.1) + 4) = -(1.21 - 4.4 + 4) = -0.79 $\n",
    "\n",
    "4. **Move to the Best Neighbor**:\n",
    "   - Among $ f(1) = -1 $, $ f(0.9) = -0.21 $, and $ f(1.1) = -0.79 $, the best value is $ f(0.9) = -0.21 $.\n",
    "   - Update the current solution: $ x = 0.9 $.\n",
    "\n",
    "5. **Repeat**:\n",
    "   - Generate new neighbors for $ x = 0.9 $:\n",
    "     - $ x_{\\text{left}} = 0.9 - 0.1 = 0.8 $\n",
    "     - $ x_{\\text{right}} = 0.9 + 0.1 = 1.0 $\n",
    "   - Evaluate $ f(x) $:\n",
    "     - $ f(0.9) = -0.21 $\n",
    "     - $ f(0.8) = -(0.8^2 - 4(0.8) + 4) = -(0.64 - 3.2 + 4) = -0.04 $\n",
    "     - $ f(1.0) = -(1.0^2 - 4(1.0) + 4) = -(1 - 4 + 4) = -1 $\n",
    "   - Move to $ x = 0.8 $, where $ f(0.8) = -0.04 $.\n",
    "\n",
    "6. **Continue Until Convergence**:\n",
    "   - Repeat the process until no better neighbor is found. Eventually, the algorithm will converge to $ x = 2 $, where $ f(2) = 0 $, the global maximum.\n",
    "\n",
    "---\n",
    "\n",
    "### Pseudocode for Maximizing a Function:\n",
    "```python\n",
    "def hill_climbing_maximize(f, x_start, step_size, max_iterations):\n",
    "    current_x = x_start\n",
    "    current_value = f(current_x)\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Generate neighbors\n",
    "        left_neighbor = current_x - step_size\n",
    "        right_neighbor = current_x + step_size\n",
    "        \n",
    "        # Evaluate the function at the neighbors\n",
    "        left_value = f(left_neighbor)\n",
    "        right_value = f(right_neighbor)\n",
    "        \n",
    "        # Find the best neighbor\n",
    "        if left_value > current_value and left_value >= right_value:\n",
    "            current_x = left_neighbor\n",
    "            current_value = left_value\n",
    "        elif right_value > current_value:\n",
    "            current_x = right_neighbor\n",
    "            current_value = right_value\n",
    "        else:\n",
    "            # No better neighbor found; terminate\n",
    "            break\n",
    "    \n",
    "    return current_x, current_value\n",
    "\n",
    "# Example usage\n",
    "def objective_function(x):\n",
    "    return -(x**2 - 4*x + 4)\n",
    "\n",
    "x_start = 1.0\n",
    "step_size = 0.1\n",
    "max_iterations = 100\n",
    "\n",
    "best_x, best_value = hill_climbing_maximize(objective_function, x_start, step_size, max_iterations)\n",
    "print(f\"Best x: {best_x}, Best value: {best_value}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Considerations:\n",
    "1. **Step Size ($ \\Delta x $)**:\n",
    "   - A smaller step size allows for more precise exploration but may slow down convergence.\n",
    "   - A larger step size speeds up exploration but risks overshooting the optimum.\n",
    "\n",
    "2. **Local Optima**:\n",
    "   - Hill Climbing may get stuck in local optima. To address this, you can use techniques like **random restarts** or **simulated annealing**.\n",
    "\n",
    "3. **Boundary Conditions**:\n",
    "   - Ensure that the search respects the boundaries of the domain (e.g., $ x \\in [0, 5] $).\n",
    "\n",
    "4. **Stopping Criteria**:\n",
    "   - Use a maximum number of iterations or terminate when no improvement is found.\n",
    "\n",
    "---\n",
    "\n",
    "### Output for the Example:\n",
    "Running the above code will output:\n",
    "```\n",
    "Best x: 2.0, Best value: 0.0\n",
    "```\n",
    "\n",
    "This result confirms that the algorithm successfully finds the global maximum of the function.\n",
    "\n",
    "---\n",
    "\n",
    "### Final Thoughts:\n",
    "Hill Climbing is a simple yet effective method for maximizing functions, especially when the function is smooth and unimodal (has a single peak). However, for more complex functions with multiple local optima, consider combining Hill Climbing with advanced techniques like random restarts, simulated annealing, or genetic algorithms to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3955804b-719a-41af-a1f7-b9392caf7294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best x: 2.000000000000001, Best value: -0.0\n"
     ]
    }
   ],
   "source": [
    "def hill_climbing_maximize(f, x_start, step_size, max_iterations):\n",
    "    current_x = x_start\n",
    "    current_value = f(current_x)\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Generate neighbors\n",
    "        left_neighbor = current_x - step_size\n",
    "        right_neighbor = current_x + step_size\n",
    "        \n",
    "        # Evaluate the function at the neighbors\n",
    "        left_value = f(left_neighbor)\n",
    "        right_value = f(right_neighbor)\n",
    "        \n",
    "        # Find the best neighbor\n",
    "        if left_value > current_value and left_value >= right_value:\n",
    "            current_x = left_neighbor\n",
    "            current_value = left_value\n",
    "        elif right_value > current_value:\n",
    "            current_x = right_neighbor\n",
    "            current_value = right_value\n",
    "        else:\n",
    "            # No better neighbor found; terminate\n",
    "            break\n",
    "    \n",
    "    return current_x, current_value\n",
    "\n",
    "# Example usage\n",
    "def objective_function(x):\n",
    "    return -(x**2 - 4*x + 4)\n",
    "\n",
    "x_start = 1.0\n",
    "step_size = 0.1\n",
    "max_iterations = 100\n",
    "\n",
    "best_x, best_value = hill_climbing_maximize(objective_function, x_start, step_size, max_iterations)\n",
    "print(f\"Best x: {best_x}, Best value: {best_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4982eb0b-4bb5-46a7-a28a-0f05b9ece27a",
   "metadata": {},
   "source": [
    "## Exploration vs explotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b085a3-fe36-44a5-831c-6a9195d40e10",
   "metadata": {},
   "source": [
    "The concepts of **exploration** and **exploitation** are fundamental in decision-making, optimization, machine learning, and reinforcement learning. They represent a trade-off between trying new things (exploration) and leveraging what you already know works well (exploitation). Striking the right balance between these two is crucial for achieving optimal outcomes in many scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Definitions**\n",
    "\n",
    "- **Exploration**: This refers to the process of gathering more information about the environment or system by trying out new, potentially better options. Exploration is about taking risks and stepping into the unknown to discover new opportunities.\n",
    "\n",
    "- **Exploitation**: This refers to leveraging existing knowledge or resources to maximize immediate rewards. Exploitation focuses on making the most of what is already known to be effective or profitable.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. The Trade-Off**\n",
    "\n",
    "The exploration-exploitation trade-off arises because resources (e.g., time, money, computational power) are limited, and focusing too much on one can come at the expense of the other:\n",
    "\n",
    "- If you focus too much on **exploration**, you may waste resources on suboptimal options without fully capitalizing on the best-known solutions.\n",
    "- If you focus too much on **exploitation**, you risk missing out on discovering better options that could yield higher long-term rewards.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Examples Across Domains**\n",
    "\n",
    "#### **Reinforcement Learning**\n",
    "In reinforcement learning (RL), an agent interacts with an environment to maximize cumulative rewards. The exploration-exploitation trade-off is central to RL:\n",
    "\n",
    "- **Exploration**: The agent tries out different actions to learn about the environment's dynamics and potential rewards.\n",
    "- **Exploitation**: The agent uses its current knowledge to choose actions that maximize expected rewards.\n",
    "\n",
    "For example, in a game like chess:\n",
    "- Exploration: Trying out unconventional moves to see if they lead to better outcomes.\n",
    "- Exploitation: Sticking to well-known strategies that have proven successful in the past.\n",
    "\n",
    "#### **Business**\n",
    "In business, companies often face the exploration-exploitation dilemma:\n",
    "\n",
    "- **Exploration**: Investing in research and development (R&D) to create new products or enter new markets.\n",
    "- **Exploitation**: Focusing on optimizing existing products, services, or processes to maximize profits.\n",
    "\n",
    "For instance, a tech company might:\n",
    "- Explore: Experiment with cutting-edge technologies like AI or blockchain.\n",
    "- Exploit: Focus on improving and scaling their flagship product to capture market share.\n",
    "\n",
    "#### **Personal Decision-Making**\n",
    "In everyday life, individuals also balance exploration and exploitation:\n",
    "\n",
    "- **Exploration**: Trying new restaurants, hobbies, or career paths to discover what you enjoy or excel at.\n",
    "- **Exploitation**: Sticking to your favorite restaurant, hobby, or job because it brings satisfaction or success.\n",
    "\n",
    "#### **Clinical Trials**\n",
    "In medical research, the exploration-exploitation trade-off is evident in clinical trials:\n",
    "\n",
    "- **Exploration**: Testing new treatments to determine their efficacy and safety.\n",
    "- **Exploitation**: Using established treatments that are already known to work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e62fa04-ebb6-43e3-a510-e1ce4e9efdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "\n",
    "# Define the objective function (2D Gaussian)\n",
    "def objective_function(x, y):\n",
    "    return -(x**2 + y**2)\n",
    "\n",
    "# Generate random points for exploration\n",
    "def generate_random_points(num_points, bounds):\n",
    "    x = np.random.uniform(bounds[0], bounds[1], num_points)\n",
    "    y = np.random.uniform(bounds[0], bounds[1], num_points)\n",
    "    return x, y\n",
    "\n",
    "# Move points toward exploitation (closer to the optimum)\n",
    "def exploit(points, step_size=0.1):\n",
    "    x, y = points\n",
    "    x = x + step_size * (-x)  # Move toward x=0\n",
    "    y = y + step_size * (-y)  # Move toward y=0\n",
    "    return x, y\n",
    "\n",
    "# Create frames for the GIF\n",
    "frames = []\n",
    "num_points = 50\n",
    "bounds = [-5, 5]\n",
    "x, y = generate_random_points(num_points, bounds)\n",
    "\n",
    "for i in range(50):  # 50 frames\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    \n",
    "    # Plot the objective function as a heatmap\n",
    "    x_grid, y_grid = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))\n",
    "    z_grid = objective_function(x_grid, y_grid)\n",
    "    plt.contourf(x_grid, y_grid, z_grid, levels=50, cmap='viridis')\n",
    "    \n",
    "    # Scatter plot of points\n",
    "    plt.scatter(x, y, color='red', s=50, label='Points')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.title(f'Exploration vs Exploitation (Frame {i+1})')\n",
    "    plt.xlabel('X-axis')\n",
    "    plt.ylabel('Y-axis')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the frame\n",
    "    plt.savefig(f'frame_{i}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Update points for the next frame\n",
    "    if i > 10:  # Start exploitation after 10 frames\n",
    "        x, y = exploit((x, y), step_size=0.1)\n",
    "    else:  # Continue exploration\n",
    "        x, y = generate_random_points(num_points, bounds)\n",
    "    \n",
    "    frames.append(f'frame_{i}.png')\n",
    "\n",
    "# Compile frames into a GIF\n",
    "with imageio.get_writer('exploration_exploitation.gif', mode='I', duration=0.5) as writer:\n",
    "    for frame in frames:\n",
    "        image = imageio.imread(frame)\n",
    "        writer.append_data(image)\n",
    "\n",
    "print(\"GIF created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be40e50-83a7-45da-809e-f68f0ff9d8a2",
   "metadata": {},
   "source": [
    "**Next-Ascent Hill Climbing** is a variation of the **Hill Climbing** algorithm, which is a local search optimization technique used to find solutions to optimization problems. The key idea behind Next-Ascent Hill Climbing is to explore neighboring solutions and move to the first one that improves the current solution, rather than exhaustively evaluating all neighbors before making a decision.\n",
    "\n",
    "### How Next-Ascent Hill Climbing Works:\n",
    "1. **Start with an initial solution**: Begin with a random or predefined starting point in the search space.\n",
    "2. **Evaluate neighbors**: Generate all possible neighboring solutions (based on some defined neighborhood structure).\n",
    "3. **Move to the next ascent**: Iterate through the neighbors and move to the first neighbor that has a better (higher) objective function value than the current solution.\n",
    "4. **Repeat**: Continue this process until no improving neighbor can be found (i.e., you reach a local optimum).\n",
    "\n",
    "### Key Features:\n",
    "- Unlike **Steepest-Ascent Hill Climbing**, which evaluates all neighbors and selects the best one, Next-Ascent Hill Climbing stops as soon as it finds any neighbor that improves the current solution.\n",
    "- This approach can be faster because it avoids evaluating all neighbors, but it may also lead to suboptimal solutions more frequently.\n",
    "\n",
    "---\n",
    "\n",
    "### Algorithm (Pseudocode):\n",
    "```plaintext\n",
    "function NextAscentHillClimbing(problem):\n",
    "    current = initialState(problem)\n",
    "    while True:\n",
    "        neighbors = generateNeighbors(current)\n",
    "        foundBetter = False\n",
    "        for neighbor in neighbors:\n",
    "            if evaluate(neighbor) > evaluate(current):\n",
    "                current = neighbor\n",
    "                foundBetter = True\n",
    "                break  # Move to the first improving neighbor\n",
    "        if not foundBetter:\n",
    "            return current  # Local optimum reached\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages:\n",
    "1. **Efficiency**: By stopping at the first improvement, it avoids unnecessary evaluations of all neighbors, making it computationally cheaper.\n",
    "2. **Simplicity**: The algorithm is straightforward to implement and understand.\n",
    "\n",
    "---\n",
    "\n",
    "### Disadvantages:\n",
    "1. **Suboptimal Solutions**: Since it moves to the first improving neighbor, it may get stuck in a local optimum that is not the global optimum.\n",
    "2. **Limited Exploration**: It does not explore the entire neighborhood, so it might miss better solutions that are farther away in the search space.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Application:\n",
    "Suppose you are solving the **Traveling Salesman Problem (TSP)** using Next-Ascent Hill Climbing:\n",
    "1. Start with a random tour (a sequence of cities to visit).\n",
    "2. Generate neighbors by swapping two cities in the tour.\n",
    "3. Evaluate the total distance of each neighbor.\n",
    "4. Move to the first neighbor with a shorter total distance.\n",
    "5. Repeat until no improving neighbor is found.\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison with Other Variants:\n",
    "| Feature                     | Next-Ascent Hill Climbing       | Steepest-Ascent Hill Climbing   | Random-Restart Hill Climbing    |\n",
    "|-----------------------------|----------------------------------|----------------------------------|----------------------------------|\n",
    "| Neighbor Evaluation         | Stops at first improvement      | Evaluates all neighbors          | Restarts from random points     |\n",
    "| Speed                       | Faster                          | Slower                           | Depends on restarts             |\n",
    "| Risk of Local Optima        | High                            | High                             | Reduced                         |\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Next-Ascent Hill Climbing:\n",
    "- When computational resources are limited and you need a quick, approximate solution.\n",
    "- When the problem's landscape is relatively smooth, and finding the global optimum is less critical.\n",
    "- When combined with other techniques like **random restarts** to escape local optima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1acc76c-e567-492f-829e-f2c688fbee6d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf896c1-9ed7-4432-a58e-4df82ade0f36",
   "metadata": {},
   "source": [
    "**Steepest-Ascent Hill Climbing** is another variation of the **Hill Climbing** algorithm, which is a local search optimization technique used to find solutions to optimization problems. Unlike **Next-Ascent Hill Climbing**, which moves to the first improving neighbor, Steepest-Ascent Hill Climbing evaluates all neighbors and moves to the best one (the \"steepest ascent\").\n",
    "\n",
    "### How Steepest-Ascent Hill Climbing Works:\n",
    "1. **Start with an initial solution**: Begin with a random or predefined starting point in the search space.\n",
    "2. **Evaluate all neighbors**: Generate all possible neighboring solutions (based on some defined neighborhood structure) and calculate their objective function values.\n",
    "3. **Move to the steepest ascent**: Select the neighbor with the highest objective function value (i.e., the best improvement) and move to that neighbor.\n",
    "4. **Repeat**: Continue this process until no improving neighbor can be found (i.e., you reach a local optimum).\n",
    "\n",
    "---\n",
    "\n",
    "### Key Features:\n",
    "- **Exhaustive Evaluation**: Unlike Next-Ascent Hill Climbing, which stops at the first improvement, Steepest-Ascent evaluates all neighbors before deciding where to move.\n",
    "- **Greedy Approach**: It always moves to the best available neighbor, which can lead to faster convergence but also increases the risk of getting stuck in a local optimum.\n",
    "\n",
    "---\n",
    "\n",
    "### Algorithm (Pseudocode):\n",
    "```plaintext\n",
    "function SteepestAscentHillClimbing(problem):\n",
    "    current = initialState(problem)\n",
    "    while True:\n",
    "        neighbors = generateNeighbors(current)\n",
    "        bestNeighbor = None\n",
    "        bestValue = evaluate(current)\n",
    "        \n",
    "        for neighbor in neighbors:\n",
    "            neighborValue = evaluate(neighbor)\n",
    "            if neighborValue > bestValue:\n",
    "                bestNeighbor = neighbor\n",
    "                bestValue = neighborValue\n",
    "        \n",
    "        if bestNeighbor is None:  # No better neighbor found\n",
    "            return current  # Local optimum reached\n",
    "        else:\n",
    "            current = bestNeighbor  # Move to the best neighbor\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages:\n",
    "1. **Better Local Optima**: By always moving to the best neighbor, it has a higher chance of finding a better local optimum compared to Next-Ascent Hill Climbing.\n",
    "2. **Systematic Exploration**: Evaluating all neighbors ensures that no immediate improvement is missed.\n",
    "\n",
    "---\n",
    "\n",
    "### Disadvantages:\n",
    "1. **Computational Cost**: Evaluating all neighbors can be expensive, especially in large search spaces or when the neighborhood size is large.\n",
    "2. **Risk of Local Optima**: Like all hill climbing algorithms, it can get stuck in a local optimum and fail to find the global optimum.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Application:\n",
    "Suppose you are solving the **Knapsack Problem** using Steepest-Ascent Hill Climbing:\n",
    "1. Start with a random selection of items (a potential solution).\n",
    "2. Generate neighbors by adding or removing one item from the current selection.\n",
    "3. Evaluate the total value of each neighbor while ensuring the weight constraint is satisfied.\n",
    "4. Move to the neighbor with the highest total value.\n",
    "5. Repeat until no improving neighbor is found.\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison with Other Variants:\n",
    "| Feature                     | Steepest-Ascent Hill Climbing   | Next-Ascent Hill Climbing       | Random-Restart Hill Climbing    |\n",
    "|-----------------------------|----------------------------------|----------------------------------|----------------------------------|\n",
    "| Neighbor Evaluation         | Evaluates all neighbors          | Stops at first improvement      | Restarts from random points     |\n",
    "| Speed                       | Slower                           | Faster                          | Depends on restarts             |\n",
    "| Risk of Local Optima        | High                             | High                            | Reduced                         |\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Steepest-Ascent Hill Climbing:\n",
    "- When computational resources are sufficient to evaluate all neighbors.\n",
    "- When the problem's landscape is relatively smooth, and finding the global optimum is less critical.\n",
    "- When combined with other techniques like **random restarts** or **simulated annealing** to escape local optima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5f5f65-bffb-48cc-976d-c48702301973",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a1f62c-b651-4fd1-b68e-d35c771de887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f02b9f1e-329b-42b3-901b-f706a41e5443",
   "metadata": {},
   "source": [
    "**Random Mutation Hill Climbing (RMHC)** is an optimization algorithm that combines the principles of **Hill Climbing** with random mutations. It is particularly useful for exploring larger search spaces and avoiding local optima by introducing randomness into the search process. RMHC is often used in evolutionary algorithms, such as genetic algorithms, where it can be seen as a simplified version of mutation-based search.\n",
    "\n",
    "\n",
    "In **Random Mutation Hill Climbing**, instead of evaluating all neighbors (as in Steepest-Ascent Hill Climbing) or stopping at the first improvement (as in Next-Ascent Hill Climbing), the algorithm:\n",
    "1. Starts with an initial solution.\n",
    "2. Randomly mutates the current solution to generate a new candidate solution.\n",
    "3. Accepts the new solution if it improves the objective function value; otherwise, it discards the mutation.\n",
    "4. Repeats this process until a termination condition is met (e.g., a maximum number of iterations or no improvement after a certain number of attempts).\n",
    "\n",
    "This approach introduces randomness into the search process, allowing the algorithm to escape local optima more effectively than traditional hill climbing methods.\n",
    "\n",
    "---\n",
    "\n",
    "### Algorithm (Pseudocode):\n",
    "```plaintext\n",
    "function RandomMutationHillClimbing(problem, maxIterations):\n",
    "    current = initialState(problem)\n",
    "    for iteration in range(maxIterations):\n",
    "        # Generate a random mutation of the current solution\n",
    "        mutated = mutate(current)\n",
    "        \n",
    "        # Evaluate the objective function for the mutated solution\n",
    "        if evaluate(mutated) > evaluate(current):\n",
    "            current = mutated  # Accept the mutation if it improves the solution\n",
    "    \n",
    "    return current  # Return the best solution found\n",
    "```\n",
    "\n",
    "Here:\n",
    "- `mutate(current)` generates a random variation of the current solution. The nature of the mutation depends on the problem domain (e.g., flipping a bit in binary strings, swapping elements in permutations, etc.).\n",
    "- `evaluate(solution)` computes the objective function value for a given solution.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Features:\n",
    "1. **Random Exploration**: By introducing random mutations, RMHC explores the search space more broadly than deterministic hill climbing methods.\n",
    "2. **Greedy Acceptance**: Only improving mutations are accepted, ensuring that the algorithm progresses toward better solutions.\n",
    "3. **Simplicity**: The algorithm is straightforward to implement and requires minimal computational overhead compared to more complex evolutionary algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages:\n",
    "1. **Avoids Local Optima**: The random mutations allow the algorithm to explore different regions of the search space, reducing the risk of getting stuck in local optima.\n",
    "2. **Efficient for Large Search Spaces**: Since it doesn't evaluate all neighbors, RMHC can handle large or complex search spaces more efficiently than exhaustive methods like Steepest-Ascent Hill Climbing.\n",
    "3. **Easy to Implement**: The algorithm is simple and requires only basic mutation and evaluation functions.\n",
    "\n",
    "---\n",
    "\n",
    "### Disadvantages:\n",
    "1. **Slow Convergence**: Because it relies on random mutations, RMHC may take many iterations to find good solutions, especially in rugged or deceptive landscapes.\n",
    "2. **No Guarantee of Global Optimum**: Like other hill climbing methods, RMHC is not guaranteed to find the global optimum, although it has a better chance than deterministic hill climbing due to its random exploration.\n",
    "3. **Tuning Mutation Rate**: The effectiveness of RMHC depends on the mutation operator and its parameters (e.g., how \"large\" the mutations are). Poorly tuned mutations can lead to inefficient exploration.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Application:\n",
    "#### Problem: **Binary String Optimization**\n",
    "Suppose you want to maximize the number of `1`s in a binary string of length `n`.\n",
    "\n",
    "1. Start with a random binary string (e.g., `010101`).\n",
    "2. Generate a random mutation by flipping one randomly chosen bit (e.g., `010111`).\n",
    "3. Evaluate the number of `1`s in the mutated string.\n",
    "4. If the mutated string has more `1`s than the current string, accept it as the new solution.\n",
    "5. Repeat until a termination condition is met (e.g., a maximum number of iterations or no improvement after a certain number of attempts).\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison with Other Variants:\n",
    "| Feature                     | Random Mutation Hill Climbing   | Steepest-Ascent Hill Climbing   | Next-Ascent Hill Climbing       |\n",
    "|-----------------------------|----------------------------------|----------------------------------|----------------------------------|\n",
    "| Exploration                 | Random mutations                | Evaluates all neighbors          | Stops at first improvement      |\n",
    "| Speed                       | Moderate                        | Slower                           | Faster                          |\n",
    "| Risk of Local Optima        | Reduced                         | High                             | High                            |\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Random Mutation Hill Climbing:\n",
    "1. **Large Search Spaces**: RMHC is well-suited for problems with large or complex search spaces where exhaustive evaluation of neighbors is impractical.\n",
    "2. **Deceptive Landscapes**: When the objective function has many local optima, RMHC's random mutations help avoid getting trapped in suboptimal solutions.\n",
    "3. **Simple Implementation Needed**: If you need a lightweight optimization algorithm that balances exploration and exploitation without the complexity of full evolutionary algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "### Extensions and Enhancements:\n",
    "1. **Adaptive Mutation**: Adjust the mutation rate dynamically based on the progress of the algorithm. For example, increase the mutation rate when the algorithm gets stuck in a plateau.\n",
    "2. **Hybrid Approaches**: Combine RMHC with other techniques like **simulated annealing** (to accept worse solutions probabilistically) or **genetic algorithms** (to incorporate crossover operators).\n",
    "3. **Restart Strategy**: Periodically restart the algorithm from a new random solution to explore different regions of the search space.\n",
    "\n",
    "---\n",
    "**Random Mutation Hill Climbing** is a powerful yet simple optimization technique that strikes a balance between exploration and exploitation. By introducing randomness through mutations, it avoids some of the pitfalls of deterministic hill climbing methods while remaining computationally efficient. However, its reliance on randomness means it may require more iterations to converge compared to more structured approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8416b85a-1532-455a-9477-2f3bbe1fd547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
